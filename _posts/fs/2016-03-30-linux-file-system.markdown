---
layout: "post"
title: "Linux文件系统概述"
date: "2016-03-30 23:21"
categories:  ["fs"]
tag:  ["fs","linux"]
---

最近研究了linux的文件系统，一开始使用<<深入理解linux内核>>， 读了两天感觉非常吃力，读不下去了，因为这本书直接从代码开始讲，缺少对整体的分析。后来改去看<<深入linux内核架构>>，还可以，它从整体架构开始讲，然后辅助一些内核代码，读完文件系统这部分后，整体上能够对文件系统有清晰的认识，但是很多细节仍然不懂，毕竟linux内核可谓一个浩瀚的工程，各部分相互交错。

这篇博客整理了linux文件系统最核心、最容易理解的部分，鉴于能力有限，其中一定有许多经不起推敲的地方，仅代表个人观点。文中插图均来自 <<深入linux内核架构>>。

### VFS

![vfs](/assets/pic/2016/03/vfs.png)


可以这么说， vfs定义了各文件系统必须实现的接口。各文件系统负责管理硬件介质上的数据。

![vfs_data_struct](/assets/pic/2016/03/vfs_data_struct.png)

相关数据结构：

1.  task_struct: 内核维护的与进程相关的变量，其中files数组保存了进程打开的所有文件对象file_struct。在进程中，文件句柄fd用int表示，指的就是文件对象在files数组中的索引，0、1、2 指向标准输入、标准输出、标准错误。
2. file_struct: 文件对象。包含一些文件操作函数f_op, 这些函数与文件内容的读取和写入有关； 还包含一个目录项f_dentry。
3. f_dentry: 目录项。负责保存文件名和inode的对应关系。操作系统使用缓存机制，缓存了部分目录项，达到快速查找文件的目的。
4. inode: 负责维护文件元数据和指向文件内容本身。在ext文件系统中， 目录和文件都以inode的形势组织。
5. super_block: 超级块。 linux每加载一个文件系统，都会生成一个对应的超级块，它包含了文件系统所有信息，可以看作是文件系统的入口。比较重要的是它维护了文件系统上打开文件的链表和脏inode的链表，脏inode的是指修改过的文件inode链表，内核负责把脏数据刷新到磁盘上。

符号链接(软链)： 目录项dentry指向的inode的数据段是一个路径字符串，该路径又指向另一个目标inode, 当删除了目标文件inode, 软链就失效了。

硬链接: 两个或者多个dentry指向同一个inode, 在一个硬链接上执行删除操作，只是销毁了一个目录项，这个文件inode并不会销毁，因为inode维护了一个计数器，用来计算有多少硬链接指向它，只有当计数器为0时，才认为文件要被删除。

要想挂载一个文件系统，这个文件系统的实现代码要么编译进内核，要么通过内核模块的方式加载进内核。然后要把文件系统注册进内核，一个文件系统的注册信息(file_systeme_type)包含：文件系统名称、用于获得超级块的函数的地址、超级块链表。因为一个文件系统可以挂载到目录树的多个位置，所以会产生多个超级块。


### Ext2/Ext3

Ext2可以说是linux原生的文件系统，其目标就是优化和vfs的互操作； Ext3兼容Ext2,  在Ext2的基础上，添加了日志功能，能够保证系统崩溃后的数据一致性。由于对Ext3日志系统的理解过于浅薄，下文只对Ext2文件系统进行描述。

#### Ext2

我的理解是，Ext是面向机械硬盘(HHD)的，并针对其物理结构，做了大量的优化工作，原则上就是减少磁头寻道和切换扇区的时间。它把硬盘做了如下的抽象。

首先文件系统存取的基本单位是固定大小的块（block）。

![fileStruct](/assets/pic/2016/03/file_block.png)

如上图，我们希望3个文件的存储是紧密相连的，这样可以充分利用存储空间，像第一矩形那样。但是硬盘上的基本存取单位是固定的，如果紧密排列，每个文件需要有额外的数据去管理文件之间的边界。所以真实的存储方式应该是第二种情况。

磁盘存储结构如下：

![diskStruct](/assets/pic/2016/03/disk_struct.png)

磁盘的起始地址存放启动块，用于启动操作系统。其后的空间由n个块组组成。块组的结构如下：

![blockGroups](/assets/pic/2016/03/block_groups.png)

这里边的数据就明显地和VFS里定义的数据结构对应了。

1. 超级快。每个块组中都有一个相同的超级快。书中先说了这么做是为了冗余，一个块组的数据丢失，不致于导致整个文件系统不可用。后来又说内核实际上只使用第一个块组的超级块，并且会加载到内存中，让我感觉这个设计是错误的。所以这里没搞懂。
2. 组描述符。每个组描述符，是块组的meta数据，一个块组对应一个组描述符，每个块组中都存储了所有的组描述符，这是为了当要在块组之间跳跃时，可以快速找到块组的位置。组描述符中的数据包括，指向数据位图的指针，指向inode位图的指针，块组内可用inode的数量、可用数据块的数量。
3. 数据位图、inode位图。比特串，每一位对应一个数据块、inode。用比特的状态代表数据块或者inode是否可用。
4. inode表。存储所有的inode。
5. 数据块。存储文件数据。

inode保存了文件数据的数据块编号， 但是不保证一定是同一个块组。并且inode的大小是固定的，我认为这是为了能够在inode表中，快速查找。 inode存储数据块编号的块一共有15个，假设块大小是1024字节，一个块编号4字节，如果采用直接存储, inode指向的文件最大为：1024/4*15*1024 = 3932160 子节，即3.75M。  为了存储大文件，提供了“间接块”的方法。 逻辑图如下：

![indirect](/assets/pic/2016/03/indirect.png)

前12个数据块直接存储数据块编号，叫做直接数据块。如果直接数据块满了，就使用后三块，分别是指向简单间接块、二次间接块、三次间接块。即后三个块中存储的编号所指向的块并不存储文件数据，而是继续存储块编号，成为间接块，如果是简单间接块，则只通过一个间接块，二次间接会通过两个间接块。最后的间接块存储指向文件数据的数据块。根据文件大小，会先使用简单间接块，如果不满足大小，则继续使用二次间接、三次间接块。下边计算使用间接后inode指向的文件的最大值，假设块大小1024，块编号4子节：

每个块可存储的块编号为1024/4=256 个

1. 12个直接数据块：(256 \* 12) \* 1024
2. 简单间接：(256 \* 256) \* 1024
3. 二次间接：(256 \* 256 \* 256) \* 1024
4. 三次间接：(256 \* 256 \* 256 \* 256) \* 1024

把四个数值加在一起为16G

块长和最大文件长度的关系：

![blksize_filesize](/assets/pic/2016/03/blksize_filesize.png)

inode自身是不存储文件名的。文件名、文件类型这些信息存储在目录项中，ext2相关的数据结构如下：

    struct ext2_dir_entry_2 {
      __len32 inode;
      __len16 rec_len;
      __u8 name_len;
      __u8 file_type;
      char name[EXT2_NAME_LEN];
    };

比如根目录/, 它对应一个inode, 这个inode的数据块中存储的就是一个ext2_dir_entry_2的列表，通过每个ext2_dir_entry_2可以实现路径搜索。在存储ext2_dir_entry_2列表，即目录元素列表时，ext2使用了一个小技巧，可以快速查找和删除目录元素:  用rec_len代表从当前entry的rec_len字段尾部到下一个entry的rec_len首部的字节数。根据rec_len的值遍历目录项，如果要删除一个目录项，只需要修改前一个entry的rec_len值。
